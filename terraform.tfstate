{
  "version": 4,
  "terraform_version": "1.0.10",
  "serial": 1,
  "lineage": "a8e66a39-1295-3786-9520-2150b11a29c1",
  "outputs": {},
  "resources": [
    {
      "mode": "data",
      "type": "spectrocloud_cloudaccount_aws",
      "name": "account",
      "provider": "provider[\"registry.terraform.io/spectrocloud/spectrocloud\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "aws_access_key": "AKIAW5KT2C4K5JFAJT4M",
            "id": "6181c13a6ccd5372808de605",
            "name": "do-not-use-vinnie-test"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "spectrocloud_cluster_profile",
      "name": "profile",
      "provider": "provider[\"registry.terraform.io/spectrocloud/spectrocloud\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "61816b4c6ccd53580f40523f",
            "name": "DevAWS-IAAS-vinnie-copy",
            "pack": [
              {
                "manifest": [],
                "name": "centos-aws",
                "tag": "7.7",
                "type": "spectro",
                "uid": "5f5a6630c5fdaf236bbc3d17",
                "values": "# Spectro Golden images includes most of the hardening standards recommended by CIS benchmarking v1.5\n\n# Uncomment below section to\n# 1. Include custom files to be copied over to the nodes and/or\n# 2. Execute list of commands before or after kubeadm init/join is executed\n#\n#kubeadmconfig:\n#  preKubeadmCommands:\n#  - echo \"Executing pre kube admin config commands\"\n#  - update-ca-certificates\n#  - 'systemctl restart containerd; sleep 3'\n#  - 'while [ ! -S /var/run/containerd/containerd.sock ]; do echo \"Waiting for containerd...\"; sleep 1; done'\n#  postKubeadmCommands:\n#  - echo \"Executing post kube admin config commands\"\n#  files:\n#  - targetPath: /usr/local/share/ca-certificates/mycom.crt\n#    targetOwner: \"root:root\"\n#    targetPermissions: \"0644\"\n#    content: |\n#      -----BEGIN CERTIFICATE-----\n#      MIICyzCCAbOgAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl\n#      cm5ldGVzMB4XDTIwMDkyMjIzNDMyM1oXDTMwMDkyMDIzNDgyM1owFTETMBEGA1UE\n#      AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMdA\n#      nZYs1el/6f9PgV/aO9mzy7MvqaZoFnqO7Qi4LZfYzixLYmMUzi+h8/RLPFIoYLiz\n#      qiDn+P8c9I1uxB6UqGrBt7dkXfjrUZPs0JXEOX9U/6GFXL5C+n3AUlAxNCS5jobN\n#      fbLt7DH3WoT6tLcQefTta2K+9S7zJKcIgLmBlPNDijwcQsbenSwDSlSLkGz8v6N2\n#      7SEYNCV542lbYwn42kbcEq2pzzAaCqa5uEPsR9y+uzUiJpv5tDHUdjbFT8tme3vL\n#      9EdCPODkqtMJtCvz0hqd5SxkfeC2L+ypaiHIxbwbWe7GtliROvz9bClIeGY7gFBK\n#      jZqpLdbBVjo0NZBTJFUCAwEAAaMmMCQwDgYDVR0PAQH/BAQDAgKkMBIGA1UdEwEB\n#      /wQIMAYBAf8CAQAwDQYJKoZIhvcNAQELBQADggEBADIKoE0P+aVJGV9LWGLiOhki\n#      HFv/vPPAQ2MPk02rLjWzCaNrXD7aPPgT/1uDMYMHD36u8rYyf4qPtB8S5REWBM/Y\n#      g8uhnpa/tGsaqO8LOFj6zsInKrsXSbE6YMY6+A8qvv5lPWpJfrcCVEo2zOj7WGoJ\n#      ixi4B3fFNI+wih8/+p4xW+n3fvgqVYHJ3zo8aRLXbXwztp00lXurXUyR8EZxyR+6\n#      b+IDLmHPEGsY9KOZ9VLLPcPhx5FR9njFyXvDKmjUMJJgUpRkmsuU1mCFC+OHhj56\n#      IkLaSJf6z/p2a3YjTxvHNCqFMLbJ2FvJwYCRzsoT2wm2oulnUAMWPI10vdVM+Nc=\n#      -----END CERTIFICATE-----"
              },
              {
                "manifest": [],
                "name": "kubernetes",
                "tag": "1.20.11",
                "type": "spectro",
                "uid": "617e310002f113d0ac3e4dd6",
                "values": "# spectrocloud.com/enabled-presets: Kube Controller Manager:loopback-ctrlmgr,Kube Scheduler:loopback-scheduler\npack:\n  k8sHardening: True\n  #CIDR Range for Pods in cluster\n  # Note : This must not overlap with any of the host or service network\n  podCIDR: \"192.168.0.0/16\"\n  #CIDR notation IP range from which to assign service cluster IPs\n  # Note : This must not overlap with any IP ranges assigned to nodes for pods.\n  serviceClusterIpRange: \"10.96.0.0/12\"\n\n# KubeAdm customization for kubernetes hardening. Below config will be ignored if k8sHardening property above is disabled\nkubeadmconfig:\n  apiServer:\n    extraArgs:\n      # Note : secure-port flag is used during kubeadm init. Do not change this flag on a running cluster\n      secure-port: \"6443\"\n      anonymous-auth: \"true\"\n      insecure-port: \"0\"\n      profiling: \"false\"\n      disable-admission-plugins: \"AlwaysAdmit\"\n      default-not-ready-toleration-seconds: \"60\"\n      default-unreachable-toleration-seconds: \"60\"\n      enable-admission-plugins: \"AlwaysPullImages,NamespaceLifecycle,ServiceAccount,NodeRestriction,PodSecurityPolicy\"\n      audit-log-path: /var/log/apiserver/audit.log\n      audit-policy-file: /etc/kubernetes/audit-policy.yaml\n      audit-log-maxage: \"30\"\n      audit-log-maxbackup: \"10\"\n      audit-log-maxsize: \"100\"\n      authorization-mode: RBAC,Node\n      tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n    extraVolumes:\n      - name: audit-log\n        hostPath: /var/log/apiserver\n        mountPath: /var/log/apiserver\n        pathType: DirectoryOrCreate\n      - name: audit-policy\n        hostPath: /etc/kubernetes/audit-policy.yaml\n        mountPath: /etc/kubernetes/audit-policy.yaml\n        readOnly: true\n        pathType: File\n  controllerManager:\n    extraArgs:\n      profiling: \"false\"\n      terminated-pod-gc-threshold: \"25\"\n      pod-eviction-timeout: \"1m0s\"\n      use-service-account-credentials: \"true\"\n      feature-gates: \"RotateKubeletServerCertificate=true\"\n  scheduler:\n    extraArgs:\n      profiling: \"false\"\n  kubeletExtraArgs:\n    read-only-port : \"0\"\n    event-qps: \"0\"\n    feature-gates: \"RotateKubeletServerCertificate=true\"\n    protect-kernel-defaults: \"true\"\n    tls-cipher-suites: \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\"\n  files:\n    - path: hardening/audit-policy.yaml\n      targetPath: /etc/kubernetes/audit-policy.yaml\n      targetOwner: \"root:root\"\n      targetPermissions: \"0600\"\n    - path: hardening/privileged-psp.yaml\n      targetPath: /etc/kubernetes/hardening/privileged-psp.yaml\n      targetOwner: \"root:root\"\n      targetPermissions: \"0600\"\n    - path: hardening/90-kubelet.conf\n      targetPath: /etc/sysctl.d/90-kubelet.conf\n      targetOwner: \"root:root\"\n      targetPermissions: \"0600\"\n  preKubeadmCommands:\n    # For enabling 'protect-kernel-defaults' flag to kubelet, kernel parameters changes are required\n    - 'echo \"====\u003e Applying kernel parameters for Kubelet\"'\n    - 'sysctl -p /etc/sysctl.d/90-kubelet.conf'\n  postKubeadmCommands:\n    # Apply the privileged PodSecurityPolicy on the first master node ; Otherwise, CNI (and other) pods won't come up\n    - 'export KUBECONFIG=/etc/kubernetes/admin.conf'\n    # Sometimes api server takes a little longer to respond. Retry if applying the pod-security-policy manifest fails\n    - '[ -f \"$KUBECONFIG\" ] \u0026\u0026 { echo \" ====\u003e Applying PodSecurityPolicy\" ; until $(kubectl apply -f /etc/kubernetes/hardening/privileged-psp.yaml \u003e /dev/null ); do echo \"Failed to apply PodSecurityPolicies, will retry in 5s\" ; sleep 5 ; done ; } || echo \"Skipping PodSecurityPolicy for worker nodes\"'\n\n# Client configuration to add OIDC based authentication flags in kubeconfig\n#clientConfig:\n  #oidc-issuer-url: \"{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-issuer-url }}\"\n  #oidc-client-id: \"{{ .spectro.pack.kubernetes.kubeadmconfig.apiServer.extraArgs.oidc-client-id }}\"\n  #oidc-client-secret: 1gsranjjmdgahm10j8r6m47ejokm9kafvcbhi3d48jlc3rfpprhv\n  #oidc-extra-scope: profile,email"
              },
              {
                "manifest": [],
                "name": "cni-calico",
                "tag": "3.19.0",
                "type": "spectro",
                "uid": "60928424f2244233f5054410",
                "values": "manifests:\n  calico:\n\n    # IPAM type to use. Supported types are calico-ipam, host-local\n    ipamType: \"calico-ipam\"\n\n    # Should be one of CALICO_IPV4POOL_IPIP or CALICO_IPV4POOL_VXLAN\n    encapsulationType: \"CALICO_IPV4POOL_IPIP\"\n\n    # Should be one of Always, CrossSubnet, Never\n    encapsulationMode: \"Always\""
              },
              {
                "manifest": [],
                "name": "csi-aws",
                "tag": "1.0.0",
                "type": "spectro",
                "uid": "5ee9d49e6f8f6125a69828bf",
                "values": "manifests:\n  aws_ebs:\n\n    #Storage type should be one of io1, gp2, sc1, st1 types\n    #Check https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html for more details\n    storageType: \"gp2\"\n\n    #Allowed reclaim policies are Delete, Retain\n    reclaimPolicy: \"Delete\"\n\n    #Toggle for Volume expansion\n    allowVolumeExpansion: \"true\"\n\n    #Toggle for Default class\n    isDefaultClass: \"true\"\n\n    #Supported binding modes are Immediate, WaitForFirstConsumer\n    #Setting this to WaitForFirstConsumer for AWS, so that the volumes gets created in the same AZ as that of the pods\n    volumeBindingMode: \"WaitForFirstConsumer\""
              },
              {
                "manifest": [],
                "name": "nginx",
                "tag": "0.43.0",
                "type": "spectro",
                "uid": "601cd493acad826fe182bc0b",
                "values": "pack:\n  #The namespace (on the target cluster) to install this chart\n  #When not found, a new namespace will be created\n  namespace: nginx\n  spectrocloud.com/install-priority: \"0\"\ncharts:\n  ingress-nginx:\n    fullnameOverride: nginx-ingress\n    controller:\n      fullnameOverride: nginx-ingress.controller\n      name: controller\n      image:\n        repository: k8s.gcr.io/ingress-nginx/controller\n        tag: v0.43.0\n        digest: sha256:9bba603b99bf25f6d117cf1235b6598c16033ad027b143c90fa5b3cc583c5713\n        pullPolicy: IfNotPresent\n        # www-data -\u003e uid 101\n        runAsUser: 101\n        allowPrivilegeEscalation: true\n      # Configures the ports the nginx-controller listens on\n      containerPort:\n        http: 80\n        https: 443\n      # Will add custom configuration options to Nginx https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/\n      config: {}\n      ## Annotations to be added to the controller config configuration configmap\n      ##\n      configAnnotations: {}\n      # Will add custom headers before sending traffic to backends according to https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/customization/custom-headers\n      proxySetHeaders: {}\n      # Will add custom headers before sending response traffic to the client according to: https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#add-headers\n      addHeaders: {}\n      # Optionally customize the pod dnsConfig.\n      dnsConfig: {}\n      # Optionally change this to ClusterFirstWithHostNet in case you have 'hostNetwork: true'.\n      # By default, while using host network, name resolution uses the host's DNS. If you wish nginx-controller\n      # to keep resolving names inside the k8s network, use ClusterFirstWithHostNet.\n      dnsPolicy: ClusterFirst\n      # Bare-metal considerations via the host network https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#via-the-host-network\n      # Ingress status was blank because there is no Service exposing the NGINX Ingress controller in a configuration using the host network, the default --publish-service flag used in standard cloud setups does not apply\n      reportNodeInternalIp: false\n      # Required for use with CNI based kubernetes installations (such as ones set up by kubeadm),\n      # since CNI and hostport don't mix yet. Can be deprecated once https://github.com/kubernetes/kubernetes/issues/23920\n      # is merged\n      hostNetwork: false\n      ## Use host ports 80 and 443\n      ## Disabled by default\n      ##\n      hostPort:\n        enabled: false\n        ports:\n          http: 80\n          https: 443\n      ## Election ID to use for status update\n      ##\n      electionID: ingress-controller-leader\n      ## Name of the ingress class to route through this controller\n      ##\n      ingressClass: nginx\n      # labels to add to the pod container metadata\n      podLabels: {}\n      #  key: value\n      ## Security Context policies for controller pods\n      ##\n      podSecurityContext: {}\n      ## See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for\n      ## notes on enabling and using sysctls\n      ###\n      sysctls: {}\n      # sysctls:\n      #   \"net.core.somaxconn\": \"8192\"\n      ## Allows customization of the source of the IP address or FQDN to report\n      ## in the ingress status field. By default, it reads the information provided\n      ## by the service. If disable, the status field reports the IP address of the\n      ## node or nodes where an ingress controller pod is running.\n      publishService:\n        enabled: true\n        ## Allows overriding of the publish service to bind to\n        ## Must be \u003cnamespace\u003e/\u003cservice_name\u003e\n        ##\n        pathOverride: \"\"\n      ## Limit the scope of the controller\n      ##\n      scope:\n        enabled: false\n        namespace: \"\"\n      ## Allows customization of the configmap / nginx-configmap namespace\n      ##\n      configMapNamespace: \"\"\n      ## Allows customization of the tcp-services-configmap\n      ##\n      tcp:\n        configMapNamespace: \"\"\n        ## Annotations to be added to the tcp config configmap\n        annotations: {}\n      ## Allows customization of the udp-services-configmap\n      ##\n      udp:\n        configMapNamespace: \"\"\n        ## Annotations to be added to the udp config configmap\n        annotations: {}\n      # Maxmind license key to download GeoLite2 Databases\n      # https://blog.maxmind.com/2019/12/18/significant-changes-to-accessing-and-using-geolite2-databases\n      maxmindLicenseKey: \"\"\n      ## Additional command line arguments to pass to nginx-ingress-controller\n      ## E.g. to specify the default SSL certificate you can use\n      ## extraArgs:\n      ##   default-ssl-certificate: \"\u003cnamespace\u003e/\u003csecret_name\u003e\"\n      extraArgs: {}\n      ## Additional environment variables to set\n      extraEnvs: []\n      # extraEnvs:\n      #   - name: FOO\n      #     valueFrom:\n      #       secretKeyRef:\n      #         key: FOO\n      #         name: secret-resource\n      ## DaemonSet or Deployment\n      ##\n      kind: Deployment\n      ## Annotations to be added to the controller Deployment or DaemonSet\n      ##\n      annotations: {}\n      #  keel.sh/pollSchedule: \"@every 60m\"\n      ## Labels to be added to the controller Deployment or DaemonSet\n      ##\n      labels: {}\n      #  keel.sh/policy: patch\n      #  keel.sh/trigger: poll\n      # The update strategy to apply to the Deployment or DaemonSet\n      ##\n      updateStrategy: {}\n      #  rollingUpdate:\n      #    maxUnavailable: 1\n      #  type: RollingUpdate\n      # minReadySeconds to avoid killing pods before we are ready\n      ##\n      minReadySeconds: 0\n      ## Node tolerations for server scheduling to nodes with taints\n      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n      ##\n      tolerations: []\n      #  - key: \"key\"\n      #    operator: \"Equal|Exists\"\n      #    value: \"value\"\n      #    effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n      ## Affinity and anti-affinity\n      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n      ##\n      affinity: {}\n      # # An example of preferred pod anti-affinity, weight is in the range 1-100\n      # podAntiAffinity:\n      #   preferredDuringSchedulingIgnoredDuringExecution:\n      #   - weight: 100\n      #     podAffinityTerm:\n      #       labelSelector:\n      #         matchExpressions:\n      #         - key: app.kubernetes.io/name\n      #           operator: In\n      #           values:\n      #           - ingress-nginx\n      #         - key: app.kubernetes.io/instance\n      #           operator: In\n      #           values:\n      #           - ingress-nginx\n      #         - key: app.kubernetes.io/component\n      #           operator: In\n      #           values:\n      #           - controller\n      #       topologyKey: kubernetes.io/hostname\n      # # An example of required pod anti-affinity\n      # podAntiAffinity:\n      #   requiredDuringSchedulingIgnoredDuringExecution:\n      #   - labelSelector:\n      #       matchExpressions:\n      #       - key: app.kubernetes.io/name\n      #         operator: In\n      #         values:\n      #         - ingress-nginx\n      #       - key: app.kubernetes.io/instance\n      #         operator: In\n      #         values:\n      #         - ingress-nginx\n      #       - key: app.kubernetes.io/component\n      #         operator: In\n      #         values:\n      #         - controller\n      #     topologyKey: \"kubernetes.io/hostname\"\n      ## Topology spread constraints rely on node labels to identify the topology domain(s) that each Node is in.\n      ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n      ##\n      topologySpreadConstraints: []\n      # - maxSkew: 1\n      #   topologyKey: failure-domain.beta.kubernetes.io/zone\n      #   whenUnsatisfiable: DoNotSchedule\n      #   labelSelector:\n      #     matchLabels:\n      #       app.kubernetes.io/instance: ingress-nginx-internal\n      ## terminationGracePeriodSeconds\n      ## wait up to five minutes for the drain of connections\n      ##\n      terminationGracePeriodSeconds: 300\n      ## Node labels for controller pod assignment\n      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n      ##\n      nodeSelector:\n        kubernetes.io/os: linux\n      ## Liveness and readiness probe values\n      ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes\n      ##\n      livenessProbe:\n        failureThreshold: 5\n        initialDelaySeconds: 10\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n        port: 10254\n      readinessProbe:\n        failureThreshold: 3\n        initialDelaySeconds: 10\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 1\n        port: 10254\n      # Path of the health check endpoint. All requests received on the port defined by\n      # the healthz-port parameter are forwarded internally to this path.\n      healthCheckPath: /healthz\n      ## Annotations to be added to controller pods\n      ##\n      podAnnotations: {}\n      replicaCount: 1\n      minAvailable: 1\n      # Define requests resources to avoid probe issues due to CPU utilization in busy nodes\n      # ref: https://github.com/kubernetes/ingress-nginx/issues/4735#issuecomment-551204903\n      # Ideally, there should be no limits.\n      # https://engineering.indeedblog.com/blog/2019/12/cpu-throttling-regression-fix/\n      resources:\n        #  limits:\n        #    cpu: 100m\n        #    memory: 90Mi\n        requests:\n          cpu: 100m\n          memory: 90Mi\n      # Mutually exclusive with keda autoscaling\n      autoscaling:\n        enabled: false\n        minReplicas: 1\n        maxReplicas: 11\n        targetCPUUtilizationPercentage: 50\n        targetMemoryUtilizationPercentage: 50\n      autoscalingTemplate: []\n      # Custom or additional autoscaling metrics\n      # ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-custom-metrics\n      # - type: Pods\n      #   pods:\n      #     metric:\n      #       name: nginx_ingress_controller_nginx_process_requests_total\n      #     target:\n      #       type: AverageValue\n      #       averageValue: 10000m\n      # Mutually exclusive with hpa autoscaling\n      keda:\n        apiVersion: keda.sh/v1alpha1\n        # apiVersion changes with keda 1.x vs 2.x\n        # 2.x = keda.sh/v1alpha1\n        # 1.x = keda.k8s.io/v1alpha1\n        enabled: false\n        minReplicas: 1\n        maxReplicas: 11\n        pollingInterval: 30\n        cooldownPeriod: 300\n        restoreToOriginalReplicaCount: false\n        scaledObject:\n          annotations: {}\n        triggers: []\n        #     - type: prometheus\n        #       metadata:\n        #         serverAddress: http://\u003cprometheus-host\u003e:9090\n        #         metricName: http_requests_total\n        #         threshold: '100'\n        #         query: sum(rate(http_requests_total{deployment=\"my-deployment\"}[2m]))\n        behavior: {}\n      #     scaleDown:\n      #       stabilizationWindowSeconds: 300\n      #       policies:\n      #       - type: Pods\n      #         value: 1\n      #         periodSeconds: 180\n      #     scaleUp:\n      #       stabilizationWindowSeconds: 300\n      #       policies:\n      #       - type: Pods\n      #         value: 2\n      #         periodSeconds: 60\n      ## Enable mimalloc as a drop-in replacement for malloc.\n      ## ref: https://github.com/microsoft/mimalloc\n      ##\n      enableMimalloc: true\n      ## Override NGINX template\n      customTemplate:\n        configMapName: \"\"\n        configMapKey: \"\"\n      service:\n        enabled: true\n        annotations: {}\n        labels: {}\n        # clusterIP: \"\"\n        ## List of IP addresses at which the controller services are available\n        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n        ##\n        externalIPs: []\n        # loadBalancerIP: \"\"\n        loadBalancerSourceRanges: []\n        enableHttp: true\n        enableHttps: true\n        ## Set external traffic policy to: \"Local\" to preserve source IP on\n        ## providers supporting it\n        ## Ref: https://kubernetes.io/docs/tutorials/services/source-ip/#source-ip-for-services-with-typeloadbalancer\n        # externalTrafficPolicy: \"\"\n        # Must be either \"None\" or \"ClientIP\" if set. Kubernetes will default to \"None\".\n        # Ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n        # sessionAffinity: \"\"\n        # specifies the health check node port (numeric port number) for the service. If healthCheckNodePort isn’t specified,\n        # the service controller allocates a port from your cluster’s NodePort range.\n        # Ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip\n        # healthCheckNodePort: 0\n        ports:\n          http: 80\n          https: 443\n        targetPorts:\n          http: http\n          https: https\n        type: LoadBalancer\n        # type: NodePort\n        # nodePorts:\n        #   http: 32080\n        #   https: 32443\n        #   tcp:\n        #     8080: 32808\n        nodePorts:\n          http: \"\"\n          https: \"\"\n          tcp: {}\n          udp: {}\n        ## Enables an additional internal load balancer (besides the external one).\n        ## Annotations are mandatory for the load balancer to come up. Varies with the cloud service.\n        internal:\n          enabled: false\n          annotations: {}\n          # loadBalancerIP: \"\"\n          ## Restrict access For LoadBalancer service. Defaults to 0.0.0.0/0.\n          loadBalancerSourceRanges: []\n      extraContainers: []\n      ## Additional containers to be added to the controller pod.\n      ## See https://github.com/lemonldap-ng-controller/lemonldap-ng-controller as example.\n      #  - name: my-sidecar\n      #    image: nginx:latest\n      #  - name: lemonldap-ng-controller\n      #    image: lemonldapng/lemonldap-ng-controller:0.2.0\n      #    args:\n      #      - /lemonldap-ng-controller\n      #      - --alsologtostderr\n      #      - --configmap=$(POD_NAMESPACE)/lemonldap-ng-configuration\n      #    env:\n      #      - name: POD_NAME\n      #        valueFrom:\n      #          fieldRef:\n      #            fieldPath: metadata.name\n      #      - name: POD_NAMESPACE\n      #        valueFrom:\n      #          fieldRef:\n      #            fieldPath: metadata.namespace\n      #    volumeMounts:\n      #    - name: copy-portal-skins\n      #      mountPath: /srv/var/lib/lemonldap-ng/portal/skins\n      extraVolumeMounts: []\n      ## Additional volumeMounts to the controller main container.\n      #  - name: copy-portal-skins\n      #   mountPath: /var/lib/lemonldap-ng/portal/skins\n      extraVolumes: []\n      ## Additional volumes to the controller pod.\n      #  - name: copy-portal-skins\n      #    emptyDir: {}\n      extraInitContainers: []\n      ## Containers, which are run before the app containers are started.\n      # - name: init-myservice\n      #   image: busybox\n      #   command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n      admissionWebhooks:\n        annotations: {}\n        enabled: true\n        failurePolicy: Fail\n        # timeoutSeconds: 10\n        port: 8443\n        certificate: /usr/local/certificates/cert\n        key: /usr/local/certificates/key\n        namespaceSelector: {}\n        objectSelector: {}\n        service:\n          annotations: {}\n          # clusterIP: \"\"\n          externalIPs: []\n          # loadBalancerIP: \"\"\n          loadBalancerSourceRanges: []\n          servicePort: 443\n          type: ClusterIP\n        patch:\n          enabled: true\n          image:\n            repository: docker.io/jettech/kube-webhook-certgen\n            tag: v1.5.1\n            pullPolicy: IfNotPresent\n          ## Provide a priority class name to the webhook patching job\n          ##\n          priorityClassName: \"\"\n          podAnnotations: {}\n          nodeSelector: {}\n          tolerations: []\n          runAsUser: 2000\n      metrics:\n        port: 10254\n        # if this port is changed, change healthz-port: in extraArgs: accordingly\n        enabled: false\n        service:\n          annotations: {}\n          # prometheus.io/scrape: \"true\"\n          # prometheus.io/port: \"10254\"\n          # clusterIP: \"\"\n          ## List of IP addresses at which the stats-exporter service is available\n          ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n          ##\n          externalIPs: []\n          # loadBalancerIP: \"\"\n          loadBalancerSourceRanges: []\n          servicePort: 9913\n          type: ClusterIP\n        serviceMonitor:\n          enabled: false\n          additionalLabels: {}\n          namespace: \"\"\n          namespaceSelector: {}\n          # Default: scrape .Release.Namespace only\n          # To scrape all, use the following:\n          # namespaceSelector:\n          #   any: true\n          scrapeInterval: 30s\n          # honorLabels: true\n          targetLabels: []\n          metricRelabelings: []\n        prometheusRule:\n          enabled: false\n          additionalLabels: {}\n          # namespace: \"\"\n          rules: []\n      ## Improve connection draining when ingress controller pod is deleted using a lifecycle hook:\n      ## With this new hook, we increased the default terminationGracePeriodSeconds from 30 seconds\n      ## to 300, allowing the draining of connections up to five minutes.\n      ## If the active connections end before that, the pod will terminate gracefully at that time.\n      ## To effectively take advantage of this feature, the Configmap feature\n      ## worker-shutdown-timeout new value is 240s instead of 10s.\n      ##\n      lifecycle:\n        preStop:\n          exec:\n            command:\n              - /wait-shutdown\n      priorityClassName: \"\"\n    ## Rollback limit\n    ##\n    revisionHistoryLimit: 10\n    ## Default 404 backend\n    ##\n    defaultBackend:\n      ##\n      enabled: false\n      name: defaultbackend\n      image:\n        repository: k8s.gcr.io/defaultbackend-amd64\n        tag: \"1.5\"\n        pullPolicy: IfNotPresent\n        # nobody user -\u003e uid 65534\n        runAsUser: 65534\n        runAsNonRoot: true\n        readOnlyRootFilesystem: true\n        allowPrivilegeEscalation: false\n      extraArgs: {}\n      serviceAccount:\n        create: true\n        name: \"\"\n      ## Additional environment variables to set for defaultBackend pods\n      extraEnvs: []\n      port: 8080\n      ## Readiness and liveness probes for default backend\n      ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/\n      ##\n      livenessProbe:\n        failureThreshold: 3\n        initialDelaySeconds: 30\n        periodSeconds: 10\n        successThreshold: 1\n        timeoutSeconds: 5\n      readinessProbe:\n        failureThreshold: 6\n        initialDelaySeconds: 0\n        periodSeconds: 5\n        successThreshold: 1\n        timeoutSeconds: 5\n      ## Node tolerations for server scheduling to nodes with taints\n      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n      ##\n      tolerations: []\n      #  - key: \"key\"\n      #    operator: \"Equal|Exists\"\n      #    value: \"value\"\n      #    effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n      affinity: {}\n      ## Security Context policies for controller pods\n      ## See https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/ for\n      ## notes on enabling and using sysctls\n      ##\n      podSecurityContext: {}\n      # labels to add to the pod container metadata\n      podLabels: {}\n      #  key: value\n      ## Node labels for default backend pod assignment\n      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n      ##\n      nodeSelector: {}\n      ## Annotations to be added to default backend pods\n      ##\n      podAnnotations: {}\n      replicaCount: 1\n      minAvailable: 1\n      resources: {}\n      # limits:\n      #   cpu: 10m\n      #   memory: 20Mi\n      # requests:\n      #   cpu: 10m\n      #   memory: 20Mi\n      autoscaling:\n        enabled: false\n        minReplicas: 1\n        maxReplicas: 2\n        targetCPUUtilizationPercentage: 50\n        targetMemoryUtilizationPercentage: 50\n      service:\n        annotations: {}\n        # clusterIP: \"\"\n        ## List of IP addresses at which the default backend service is available\n        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n        ##\n        externalIPs: []\n        # loadBalancerIP: \"\"\n        loadBalancerSourceRanges: []\n        servicePort: 80\n        type: ClusterIP\n      priorityClassName: \"\"\n    ## Enable RBAC as per https://github.com/kubernetes/ingress/tree/master/examples/rbac/nginx and https://github.com/kubernetes/ingress/issues/266\n    rbac:\n      create: true\n      scope: false\n    # If true, create \u0026 use Pod Security Policy resources\n    # https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n    podSecurityPolicy:\n      enabled: false\n    serviceAccount:\n      create: true\n      name: \"\"\n    ## Optional array of imagePullSecrets containing private registry credentials\n    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n    imagePullSecrets: []\n    # - name: secretName\n    # TCP service key:value pairs\n    # Ref: https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/tcp\n    ##\n    tcp: {}\n    #  8080: \"default/example-tcp-svc:9000\"\n    # UDP service key:value pairs\n    # Ref: https://github.com/kubernetes/contrib/tree/master/ingress/controllers/nginx/examples/udp\n    ##\n    udp: {}"
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "spectrocloud_cluster_aws",
      "name": "cluster",
      "provider": "provider[\"registry.terraform.io/spectrocloud/spectrocloud\"]",
      "instances": [
        {
          "status": "tainted",
          "schema_version": 2,
          "attributes": {
            "backup_policy": [],
            "cloud_account_id": "6181c13a6ccd5372808de605",
            "cloud_config": [
              {
                "region": "us-west-2",
                "ssh_key_name": "default"
              }
            ],
            "cloud_config_id": null,
            "cluster_profile": [
              {
                "id": "61816b4c6ccd53580f40523f",
                "pack": []
              }
            ],
            "cluster_profile_id": null,
            "id": "61845fbbffc55b7f04327151",
            "kubeconfig": null,
            "machine_pool": [
              {
                "azs": [
                  "us-east-1a",
                  "us-east-1b"
                ],
                "control_plane": false,
                "control_plane_as_worker": false,
                "count": 1,
                "disk_size_gb": 65,
                "instance_type": "t3.large",
                "name": "worker-basic",
                "update_strategy": "RollingUpdateScaleOut"
              },
              {
                "azs": [
                  "us-east-1a",
                  "us-east-1b"
                ],
                "control_plane": true,
                "control_plane_as_worker": true,
                "count": 1,
                "disk_size_gb": 62,
                "instance_type": "t3.large",
                "name": "master-pool",
                "update_strategy": "RollingUpdateScaleOut"
              }
            ],
            "name": "vinnie-tf-test",
            "os_patch_after": null,
            "os_patch_on_boot": null,
            "os_patch_schedule": null,
            "pack": [],
            "scan_policy": [],
            "tags": [
              "department:devops",
              "dev",
              "owner:vinnie"
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjozNjAwMDAwMDAwMDAwLCJkZWxldGUiOjM2MDAwMDAwMDAwMDAsInVwZGF0ZSI6MzYwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMiJ9",
          "dependencies": [
            "data.spectrocloud_cloudaccount_aws.account",
            "data.spectrocloud_cluster_profile.profile"
          ]
        }
      ]
    }
  ]
}
